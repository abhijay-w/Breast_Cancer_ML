library("methods")
result <- xmlParse(file= "input1.xml")
print(result)
# Give the input file name to the function.
result <- xmlParse(file = "input.xml")
print(result)
# Exract the root node form the xml file.
rootnode <- xmlRoot(result)
# Find number of nodes in the root.
rootsize <- xmlSize(rootnode)
print(rootsize)
#XML to Dataframe
print(rootnode[1])
xmldata = xmlToDataFrame("input.xml")
print(xmldata)
install.packages("XML")
library("XML")
library("methods")
result <- xmlParse(file= "input1.xml")
print(result)
# Give the input file name to the function.
result <- xmlParse(file = "input.xml")
print(result)
# Exract the root node form the xml file.
rootnode <- xmlRoot(result)
# Find number of nodes in the root.
rootsize <- xmlSize(rootnode)
print(rootsize)
#XML to Dataframe
print(rootnode[1])
xmldata = xmlToDataFrame("input.xml")
print(xmldata)
install.packages("XML")
library("XML")
library("methods")
result <- xmlParse(file= "input1.xml")
print(result)
# Give the input file name to the function.
result <- xmlParse(file = "input.xml")
print(result)
# Exract the root node form the xml file.
rootnode <- xmlRoot(result)
# Find number of nodes in the root.
rootsize <- xmlSize(rootnode)
print(rootsize)
#XML to Dataframe
print(rootnode[1])
xmldata = xmlToDataFrame("input.xml")
print(xmldata)
# Give the input file name to the function.
result <- xmlParse(file = "input.xml")
print(result)
# Exract the root node form the xml file.
rootnode <- xmlRoot(result)
# Find number of nodes in the root.
rootsize <- xmlSize(rootnode)
print(rootsize)
#XML to Dataframe
print(rootnode[1])
xmldata = xmlToDataFrame("input.xml")
print(xmldata)
xmldata = xmlToDataFrame(file = "input.xml")
library("XML")
library("methods")
result <- xmlParse(file = "input.xml")
print(result)
install.packages("XML")
install.packages("XML")
install.packages("XML")
library("XML")
library("methods")
result <- xmlParse(file = "input.xml")
print(result)
# Give the input file name to the function.
result <- xmlParse(file = input.xml)
# Give the input file name to the function.
result <- xmlParse(file = 'input.xml')
print(1
Rick
623.3
1/1/2012
IT
2
Dan
515.2
9/23/2013
Operations
3
Michelle
611
11/15/2014
IT
4
Ryan
729
5/11/2014
HR
5
Gary
843.25
3/27/2015
Finance
6
Nina
578
5/21/2013
IT
7
Simon
632.8
7/30/2013
Operations
8
Guru
722.5
6/17/2014
Finance)
print("1
Rick
623.3
1/1/2012
IT
2
Dan
515.2
9/23/2013
Operations
3
Michelle
611
11/15/2014
IT
4
Ryan
729
5/11/2014
HR
5
Gary
843.25
3/27/2015
Finance
6
Nina
578
5/21/2013
IT
7
Simon
632.8
7/30/2013
Operations
8
Guru
722.5
6/17/2014
Finance")
# Give the input file name to the function.
result <- xmlParse(file = "input.xml")
result <- xmlParse(file = "input.xml")
result <- xmlParse(file = "input.xml")
result <- xmlParse(file = "input.xml")
result <- xmlParse(file = "C:/Users/Abhijay/Desktop/SEM-5/R/input.xml")
print(result)
library("XML")
library("methods")
# Give the input file name to the function.
result <- xmlParse(file = "C:/Users/Abhijay/Desktop/SEM-5/R/input.xml")
print(result)
# Exract the root node form the xml file.
rootnode <- xmlRoot(result)
# Find number of nodes in the root.
rootsize <- xmlSize(rootnode)
print(rootsize)
#XML to Dataframe
print(rootnode[1])
xmldata = xmlToDataFrame("input.xml")
print(xmldata)
library("XML")
library("methods")
# Give the input file name to the function.
result <- xmlParse(file = "C:/Users/Abhijay/Desktop/SEM-5/R/input.xml")
print(result)
# Exract the root node form the xml file.
rootnode <- xmlRoot(result)
# Find number of nodes in the root.
rootsize <- xmlSize(rootnode)
print(rootsize)
#XML to Dataframe
print(rootnode[1])
xmldata = xmlToDataFrame("C:/Users/Abhijay/Desktop/SEM-5/R/input.xml")
print(xmldata)
getwd()
1:10
class(1:10)
typeof(1:10)
#Sequence
print(seq(70,80))
#Average
print(mean(10:20))
#Submission
print(sum(20:40))
#Function to find Factorial of a given number
fact <-function(n){
fac = 1
while(n>0){
fac=fac*n
n = n-1
}
print(fac)
}
fact(5)
fact(3)
#Without Argument
new.function <- function() {
for(i in 1:5) {
print(i^2)
}
}
#Lazy evaluation
new <- function(a,b){
print(a*a)
print(a*b)
}
new(2)
X = iris[,1:4]
print(X)
rs(list = ls())
rl(list = ls())
rm(list = ls())
X = iris[,1:4]
print(X)
#Outlier
dens(X,k=4,C=1)
Plot(X)
install.packages("pomp")
#Outlier
dens(X,k=4,C=1)
Plot(X)
depthout(X, cutoff= 0.05)
library("pomp")
dens(X,k=4,C=1)
Plot(X)
install.packages("OutlierDetection")
dens(X,k=4,C=1)
Plot(X)
library("OutlierDetection")
dens(X,k=4,C=1)
Plot(X)
depthout(X, cutoff= 0.05)
X = iris[,1:4]
library("OutlierDetection")
#Outlier
dens(X,k=4,C=1)
Plot(X)
X = iris[,1:4]
library("OutlierDetection")
#Outlier
dens(X,k=4,C=1)
Plot(X)
library("OutlierDetection")
library("methods")
#Outlier
dens(X,k=4,C=1)
Plot(X)
install.packages("methods")
install.packages("methods")
install.packages("methods")
install.packages("methods")
install.packages("methods")
library("OutlierDetection")
library("methods")
#Outlier
dens(X,k=4,C=1)
Plot(X)
library(methods)
library(ggplot2)
library(OutlierDetection)
library(methods)
library(ggplot2)
#Outlier
dens(X,k=4,C=1)
Plot(X)
dens(X,k=4,C=1)
ggplot(X)
dens(X,k=4,C=1)
plot(X)
depthout(X, cutoff= 0.05)
dens(X,k=4,C=1)
depthout(X, cutoff= 0.05)
library(rfm)
library(magrittr)               # Install dplyr R package
library(dplyr)
library(lubridate)
library(rfm)
library(magrittr)
library(dplyr)
library(lubridate)
analysis_date <- lubridate::as_date('2006-12-31')
rfm_result <- rfm_table_order(rfm_data_orders, customer_id, order_date, revenue, analysis_date)
df = rfm_result
install.packages("lubridate")
library(rfm)
library(magrittr)
library(dplyr)
library(lubridate)
analysis_date <- lubridate::as_date('2006-12-31')
rfm_result <- rfm_table_order(rfm_data_orders, customer_id, order_date, revenue, analysis_date)
rfm_result
rfm_table_customer()
rfm_table_customer
rfm_data_orders
analysis_date <- lubridate::as_date('2006-12-31')
rb
rfm_result
analysis_date <- lubridate::as_date('2007-01-01')
rfm_result <- rfm_table_customer(rfm_data_customer, customer_id, number_of_orders,
recency_days, revenue, analysis_date)
rfm_result
library(rfm)
library(magrittr)
library(dplyr)
library(lubridate)
install.packages('lubridate')
library(rfm)
library(magrittr)
library(dplyr)
library(lubridate)
analysis_date <- lubridate::as_date('2007-01-01')
rfm_result <- rfm_table_customer(rfm_data_customer, customer_id, number_of_orders,
recency_days, revenue, analysis_date)
rfm_result
getwd(
)
#Model Evaluation
model_list <- list(logisic = model_logreg_df, rf = model_rf_df, knn=model_knn_df,
SVM_with_PCA = model_svm_pca_df,Neural_with_LDA = model_nnetlda_df)
#Model Evaluation
model_list <- list(logisic = model_logreg_df, rf = model_rf_df, knn=model_knn_df,
SVM_with_PCA = model_svm_pca_df,Neural_with_LDA = model_nnetlda_df)
summary(results)
results <- map_df(cm_list, function(x) x$byClass) %>% as_tibble() %>%
mutate(stat = names(cm_rf_df$byClass))
summary(results)
setwd("C:/Users/Abhijay/Desktop/Cancer")
getwd()
#Loading Packages
library(caret)
library(ggfortify)
library(dplyr)
library(tidyverse)
library(magrittr)
#Loading dataset
df <- read.csv("data.csv")
# the 33 column is not right
df[,33] <- NULL
# This is defintely an most important step:
# Check for appropriate class on each of the variable.
glimpse(df)
df$diagnosis <- as.factor(df$diagnosis)
#Missing values
map_int(df, function(.x) sum(is.na(.x)))
round(prop.table(table(df$diagnosis)), 2)
#Checking correlations
df_corr <- cor(df %>% select(-id, -diagnosis))
corrplot::corrplot(df_corr, order = "hclust", tl.cex = 1, addrect = 8)
# The findcorrelation() function from caret package remove highly correlated predictors
# based on whose correlation is above 0.9. This function uses a heuristic algorithm
# to determine which variable should be removed instead of selecting blindly
df2 <- df %>% select(-findCorrelation(df_corr, cutoff = 0.9))
ncol(df2)
df$diagnosis <- as.factor(df$diagnosis)
round(prop.table(table(df$diagnosis)), 2)
#PCA analysis
preproc_pca_df <- prcomp(df %>% select(-id, -diagnosis), scale = TRUE, center = TRUE)
summary(preproc_pca_df)
#PCA analysis
preproc_pca_df <- prcomp(df %>% select(-id, -diagnosis), scale = TRUE, center = TRUE)
summary(preproc_pca_df)
# Calculate the proportion of variance explained
pca_df_var <- preproc_pca_df$sdev^2
pve_df <- pca_df_var / sum(pca_df_var)
cum_pve <- cumsum(pve_df)
pve_table <- tibble(comp = seq(1:ncol(df %>% select(-id, -diagnosis))), pve_df, cum_pve)
pve_table
#Ploting
ggplot(pve_table, aes(x = comp, y = cum_pve)) +
geom_point() +
geom_abline(intercept = 0.95, color = "red", slope = 0) +
labs(x = "Number of components", y = "Cumulative Variance")
#Most influential variables for the first 2 components
pca_df <- as_tibble(preproc_pca_df$x)
ggplot(pca_df, aes(x = PC1, y = PC2, col = df$diagnosis)) + geom_point()
autoplot(preproc_pca_df, data = df,  colour = 'diagnosis',
loadings = FALSE, loadings.label = TRUE, loadings.colour = "blue")
#For df2
preproc_pca_df2 <- prcomp(df2, scale = TRUE, center = TRUE)
summary(preproc_pca_df2)
#For df2
preproc_pca_df2 <- prcomp(df2, scale = TRUE, center = TRUE)
summary(preproc_pca_df2)
pca_df2_var <- preproc_pca_df2$sdev^2
pca_df2_var <- preproc_pca_df2$sdev^2
# proportion of variance explained
pve_df2 <- pca_df2_var / sum(pca_df2_var)
cum_pve_df2 <- cumsum(pve_df2)
pve_table_df2 <- tibble(comp = seq(1:ncol(df2)), pve_df2, cum_pve_df2)
ggplot(pve_table_df2, aes(x = comp, y = cum_pve_df2)) +
geom_point() +
geom_abline(intercept = 0.95, color = "red", slope = 0) +
labs(x = "Number of components", y = "Cumulative Variance")
#LDA
preproc_lda_df <- MASS::lda(diagnosis ~., data = df, center = TRUE, scale = TRUE)
preproc_lda_df
# Making a df out of the LDA for visualization purpose.
predict_lda_df <- predict(preproc_lda_df, df)$x %>%
as_tibble() %>%
cbind(diagnosis = df$diagnosis)
glimpse(predict_lda_df)
# Making a df out of the LDA for visualization purpose.
predict_lda_df <- predict(preproc_lda_df, df)$x %>%
as_tibble() %>%
cbind(diagnosis = df$diagnosis)
glimpse(predict_lda_df)
#Model the data
set.seed(1815)
df3 <- cbind(diagnosis = df$diagnosis, df2)
df_sampling_index <- createDataPartition(df3$diagnosis, times = 1, p = 0.8, list = FALSE)
df_training <- df3[df_sampling_index, ]
df_testing <-  df3[-df_sampling_index, ]
df_control <- trainControl(method="cv",
number = 15,
classProbs = TRUE,
summaryFunction = twoClassSummary)
#Logistic Regression
model_logreg_df <- train(diagnosis ~., data = df_training, method = "glm",
metric = "ROC", preProcess = c("scale", "center"),
trControl = df_control)
prediction_logreg_df <- predict(model_logreg_df, df_testing)
cm_logreg_df <- confusionMatrix(prediction_logreg_df, df_testing$diagnosis, positive = "M")
cm_logreg_df
#Random Forest
model_rf_df <- train(diagnosis ~., data = df_training,
method = "rf",
metric = 'ROC',
trControl = df_control)
prediction_rf_df <- predict(model_rf_df, df_testing)
cm_rf_df <- confusionMatrix(prediction_rf_df, df_testing$diagnosis, positive = "M")
cm_rf_df
plot(model_rf_df)
plot(model_rf_df$finalModel)
plot(model_rf_df$finalModel)
randomForest::varImpPlot(model_rf_df$finalModel, sort = TRUE,
n.var = 10, main = "The 10 variables with the most predictive power")
#KNN
model_knn_df <- train(diagnosis ~., data = df_training,
method = "knn",
metric = "ROC",
preProcess = c("scale", "center"),
trControl = df_control,
tuneLength =31)
plot(model_knn_df)
prediction_knn_df <- predict(model_knn_df, df_testing)
cm_knn_df <- confusionMatrix(prediction_knn_df, df_testing$diagnosis, positive = "M")
cm_knn_df
#SVM with PCA
set.seed(1815)
df_control_pca <- trainControl(method="cv",
number = 15,
preProcOptions = list(thresh = 0.9), # threshold for pca preprocess
classProbs = TRUE,
summaryFunction = twoClassSummary)
model_svm_pca_df <- train(diagnosis~.,
df_training, method = "svmLinear", metric = "ROC",
preProcess = c('center', 'scale', "pca"),
trControl = df_control_pca)
prediction_svm_pca_df <- predict(model_svm_pca_df, df_testing)
cm_svm_pca_df <- confusionMatrix(prediction_svm_pca_df, df_testing$diagnosis, positive = "M")
cm_svm_pca_df
#SVM with PCA
set.seed(1815)
df_control_pca <- trainControl(method="cv",
number = 15,
preProcOptions = list(thresh = 0.9), # threshold for pca preprocess
classProbs = TRUE,
summaryFunction = twoClassSummary)
model_svm_pca_df <- train(diagnosis~.,
df_training, method = "svmLinear", metric = "ROC",
preProcess = c('center', 'scale', "pca"),
trControl = df_control_pca)
prediction_svm_pca_df <- predict(model_svm_pca_df, df_testing)
cm_svm_pca_df <- confusionMatrix(prediction_svm_pca_df, df_testing$diagnosis, positive = "M")
cm_svm_pca_df
#Neural Network
lda_training <- predict_lda_df[df_sampling_index, ]
lda_testing <- predict_lda_df[-df_sampling_index, ]
model_nnetlda_df <- train(diagnosis ~., lda_training,
method = "nnet",
metric = "ROC",
preProcess = c("center", "scale"),
tuneLength = 10,
trace = FALSE,
trControl = df_control)
prediction_nnetlda_df <- predict(model_nnetlda_df, lda_testing)
cm_nnetlda_df <- confusionMatrix(prediction_nnetlda_df, lda_testing$diagnosis, positive = "M")
cm_nnetlda_df
#Model Evaluation
model_list <- list(logisic = model_logreg_df, rf = model_rf_df, knn=model_knn_df,
SVM_with_PCA = model_svm_pca_df,Neural_with_LDA = model_nnetlda_df)
results <- resamples(model_list)
summary(results)
bwplot(results, metric = "ROC")
